{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "import scipy.sparse\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load doc list\n",
    "with open('doc_list.txt') as f:\n",
    "    doc_list = f.read().splitlines()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load doc from list\n",
    "if load_from_file != True:\n",
    "  docs_counter = []\n",
    "  words = set()\n",
    "  for doc in tqdm(doc_list):\n",
    "      with open('docs/' + doc + '.txt') as f:\n",
    "          doc_words = f.read().split()\n",
    "          docs_counter.append(Counter(doc_words))\n",
    "          words = words.union(set(doc_words))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load query list\n",
    "with open('query_list.txt') as f:\n",
    "    query_list = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load query from list\n",
    "queries = []\n",
    "queries_words = set()\n",
    "for query in tqdm(query_list):\n",
    "    with open('queries/' + query + '.txt') as f:\n",
    "        query_words = f.read().split()\n",
    "        queries.append(query_words)\n",
    "        if load_from_file == False:\n",
    "            words = words.union(set(query_words))\n",
    "            queries_words = queries_words.union(set(query_words))\n",
    "\n",
    "if load_from_file:\n",
    "    # load query words from file\n",
    "    with open('query_word_list.txt') as f:\n",
    "        queries_words = f.read().split()\n",
    "else:\n",
    "    # save query words\n",
    "    with open('query_word_list.txt', 'w') as f:\n",
    "        f.write(' '.join(queries_words))\n",
    "    \n",
    "    queries_words = list(queries_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_from_file:\n",
    "    # load words dict from file\n",
    "    with open('word_list.txt') as f:\n",
    "        words = f.read().split()\n",
    "else:\n",
    "    # save words\n",
    "    with open('word_list.txt', 'w') as f:\n",
    "        f.write(' '.join(words))\n",
    "\n",
    "    words = list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_amount = len(doc_list)\n",
    "words_amount = len(words)\n",
    "query_word_amount = len(queries_words)\n",
    "\n",
    "print(docs_amount, words_amount, query_word_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document length\n",
    "if load_from_file:\n",
    "    docs_len = np.load('docs_len.npy')\n",
    "else:\n",
    "    docs_len = []\n",
    "\n",
    "    for j in tqdm(range(docs_amount)):\n",
    "        docs_len.append(sum(docs_counter[j].values()))\n",
    "\n",
    "    docs_len = np.array(docs_len)\n",
    "    np.save('docs_len', docs_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all words count in documents and probability\n",
    "if load_from_file:\n",
    "    cwd = scipy.sparse.load_npz('cwd.npz')\n",
    "    pwd = scipy.sparse.load_npz('pwd.npz')\n",
    "else:\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    cwd_data = []\n",
    "    pwd_data = []\n",
    "\n",
    "    for j in tqdm(range(docs_amount)):\n",
    "        doc_len = docs_len[j]\n",
    "\n",
    "        for i in range(words_amount):\n",
    "            word_count = docs_counter[j][words[i]]\n",
    "            if word_count != 0:\n",
    "                indices.append(i)\n",
    "                cwd_data.append(word_count)\n",
    "                pwd_data.append(word_count / doc_len)\n",
    "        indptr.append(len(indices))\n",
    "\n",
    "    cwd = scipy.sparse.csr_matrix((cwd_data, indices, indptr), dtype=np.float32).transpose()\n",
    "    pwd = scipy.sparse.csr_matrix((pwd_data, indices, indptr), dtype=np.float32).transpose()\n",
    "\n",
    "    scipy.sparse.save_npz('cwd', cwd)\n",
    "    scipy.sparse.save_npz('pwd', pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process slim words\n",
    "if load_from_file:\n",
    "    with open('slim_word_list_10000.txt') as f:\n",
    "        slim_words = f.read().split()\n",
    "else:\n",
    "    words_count_list = []\n",
    "    for word_row in tqdm(cwd):\n",
    "        words_count_list.append(word_row.sum())\n",
    "        \n",
    "    most_word_index = np.flip(np.argsort(words_count_list))\n",
    "\n",
    "    slim_words_amount = 10000\n",
    "    slim_words = []\n",
    "    for i in range(slim_words_amount):\n",
    "        slim_words.append(words[most_word_index[i]])\n",
    "    \n",
    "    slim_words = slim_words + queries_words\n",
    "    slim_words = list(set(slim_words))\n",
    "\n",
    "    slim_words_amount = len(slim_words)\n",
    "\n",
    "    # save slim words\n",
    "    with open('slim_word_list_10000.txt', 'w') as f:\n",
    "        f.write(' '.join(slim_words))\n",
    "\n",
    "# update slim words amount\n",
    "slim_words_amount = len(slim_words)\n",
    "print(slim_words_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slim words count in documents and probability\n",
    "if load_from_file:\n",
    "    slim_cwd = scipy.sparse.load_npz('slim_cwd_10000.npz').A\n",
    "    slim_pwd = scipy.sparse.load_npz('slim_pwd_10000.npz').A\n",
    "else:\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    cwd_data = []\n",
    "    pwd_data = []\n",
    "\n",
    "    for j in tqdm(range(docs_amount)):\n",
    "        doc_len = docs_len[j]\n",
    "        for i in range(slim_words_amount):\n",
    "            word_count = docs_counter[j][slim_words[i]]\n",
    "            if word_count != 0:\n",
    "                indices.append(i)\n",
    "                cwd_data.append(word_count)\n",
    "                pwd_data.append(word_count / doc_len)\n",
    "        indptr.append(len(indices))\n",
    "\n",
    "    slim_cwd = scipy.sparse.csr_matrix((cwd_data, indices, indptr), dtype=np.float32).transpose()\n",
    "    slim_pwd = scipy.sparse.csr_matrix((pwd_data, indices, indptr), dtype=np.float32).transpose()\n",
    "\n",
    "    scipy.sparse.save_npz('slim_cwd_10000', slim_cwd)\n",
    "    scipy.sparse.save_npz('slim_pwd_10000', slim_pwd)\n",
    "\n",
    "    slim_cwd = slim_cwd.A\n",
    "    slim_pwd = slim_pwd.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# background language model\n",
    "bg = []\n",
    "bg_model_cd = docs_len.sum()\n",
    "\n",
    "for word_row in tqdm(slim_cwd):\n",
    "    bg.append(word_row.sum() / bg_model_cd)\n",
    "\n",
    "bg = np.array(bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def nb_E_step(pwt, ptd, cwd, topic_amount, word_amount, doc_amount):\n",
    "    # empty matrix\n",
    "    ptwd = np.empty((topic_amount, word_amount, doc_amount))\n",
    "\n",
    "    # Common Denominator\n",
    "    # ptwd_CD = np.dot(pwt, ptd) \n",
    "\n",
    "    for i in range(word_amount):\n",
    "        for j in range(doc_amount):\n",
    "            if cwd[i][j] != 0: \n",
    "                ptwd_CD = 0\n",
    "                for k in range(topic_amount):\n",
    "                    single_ptwd = pwt[i][k] * ptd[k][j]\n",
    "                    ptwd[k][i][j] = single_ptwd\n",
    "                    ptwd_CD += single_ptwd\n",
    "                if ptwd_CD != 0:\n",
    "                    for k in range(topic_amount):\n",
    "                        ptwd[k][i][j] /= ptwd_CD\n",
    "                else:\n",
    "                    ptwd[:,i,j] = 0\n",
    "            else:\n",
    "                ptwd[:,i,j] = 0\n",
    "    return ptwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def nb_M_step(ptwd, cwd, docs_len, topic_amount, word_amount, doc_amount):\n",
    "    # empty matrix\n",
    "    pwt = np.empty((word_amount, topic_amount))\n",
    "    ptd = np.empty((topic_amount, doc_amount))\n",
    "\n",
    "    for k in range(topic_amount):\n",
    "        single_topic_wd = np.multiply(cwd, ptwd[k])\n",
    "\n",
    "        # p(w/t)\n",
    "        single_wt_sum = single_topic_wd.sum()\n",
    "        if single_wt_sum != 0:\n",
    "            for i in range(word_amount):\n",
    "                pwt[i][k] = single_topic_wd[i].sum() / single_wt_sum\n",
    "        else:\n",
    "            pwt[:,k] = 1 / slim_words_amount\n",
    "\n",
    "        # p(t/d)\n",
    "        for j in range(doc_amount):\n",
    "            ptd[k][j] = single_topic_wd[:,j].sum() / docs_len[j]\n",
    "            # ptd[k][j] = single_topic_wd[:,j].sum() / cwd[:,j].sum()\n",
    "    \n",
    "    # # norm to 1\n",
    "    # for k in range(topic_k):\n",
    "    #     if np.isnan(pwt[:,k].sum()):\n",
    "    #         print(times, \"norm \", k)\n",
    "    #     pwt[:,k] /= pwt[:,k].sum()\n",
    "    # for j in range(doc_amount):\n",
    "    #     deno = ptd[:,j].sum()\n",
    "    #     if deno != 0:\n",
    "    #         ptd[:,j] /= deno\n",
    "    #     else:\n",
    "    #         ptd[:,j].fill(1 / topic_amount) \n",
    "    \n",
    "    return pwt, ptd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def nb_loss(times, cwd, pwt, ptd):\n",
    "    loss = np.multiply(cwd, np.log(np.dot(pwt, ptd))).sum()\n",
    "    print(\"\\nStep\", times, \"loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic\n",
    "\n",
    "topic_k = 48\n",
    "EPOCH = 30\n",
    "alpha = 0.7\n",
    "beta = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM Step Initial (normal)\n",
    "pwt = np.random.random(size = (slim_words_amount, topic_k))\n",
    "\n",
    "for k in range(topic_k):\n",
    "    pwt[:,k] /= pwt[:,k].sum()\n",
    "\n",
    "ptd = np.full((topic_k, docs_amount), 1 / topic_k)\n",
    "\n",
    "ptwd = np.empty((topic_k, slim_words_amount, docs_amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(EPOCH)):\n",
    "    # nb_E_step(pwt, ptd, cwd, topic_amount, word_amount, doc_amount)\n",
    "    ptwd = nb_E_step(pwt, ptd, slim_cwd, topic_k, slim_words_amount, docs_amount)\n",
    "    # nb_M_step(ptwd, cwd, docs_len, topic_amount, word_amount, doc_amount)\n",
    "    pwt, ptd = nb_M_step(ptwd, slim_cwd, docs_len, topic_k, slim_words_amount, docs_amount)\n",
    "    # nb_loss(times, cwd, pwt, ptd)\n",
    "    # nb_loss(i + 1, slim_cwd, pwt, ptd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plsa_EM_final = np.matmul(pwt, ptd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plsa_EM_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_result = []\n",
    "\n",
    "for query in tqdm(queries):\n",
    "    query_result = []\n",
    "    for doc_index in range(docs_amount):\n",
    "        plsa_result = 1\n",
    "        for word in query:\n",
    "            word_index = slim_words.index(word)\n",
    "            unigram_pwd = slim_pwd[word_index][doc_index]\n",
    "            plsa_result = plsa_result * (alpha * unigram_pwd + beta * plsa_EM_final[word_index][doc_index] + (1 - alpha - beta) * bg[word_index])\n",
    "            \n",
    "            # plsa_result = plsa_result * (alpha * unigram_pwd + (1 - alpha - beta) * bg[word_index])\n",
    "        query_result.append(plsa_result)\n",
    "    queries_result.append(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort and export result\n",
    "sim_df = pd.DataFrame(queries_result)\n",
    "sim_df = sim_df.transpose()\n",
    "sim_df.index = doc_list\n",
    "sim_df.columns = query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "now = datetime.datetime.now()\n",
    "save_filename = 'results/result' + '_' + 'topic' + str(topic_k) + '_EPOCH' + str(EPOCH) + '_a' + str(alpha) + '_b' + str(beta) + '_word'+ str(slim_words_amount) + now.strftime(\"_%y%m%d_%H%M\") + '.txt'\n",
    "print(save_filename)\n",
    "\n",
    "with open(save_filename, 'w') as f:\n",
    "    f.write('Query,RetrievedDocuments\\n')\n",
    "    for query in query_list:\n",
    "        f.write(query + \",\")\n",
    "        query_sim_df = sim_df[query].sort_values(ascending=False)\n",
    "        f.write(' '.join(query_sim_df[:1000].index.to_list()) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_pwt = scipy.sparse.csr_matrix(pwt)\n",
    "sparse_ptd = scipy.sparse.csr_matrix(ptd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('sparse_pwt' + '_' + 'topic' + str(topic_k) + '_EPOCH' + str(EPOCH) + '_word'+ str(slim_words_amount) + now.strftime(\"_%y%m%d_%H%M\"), sparse_pwt)\n",
    "scipy.sparse.save_npz('sparse_ptd' + '_' + 'topic' + str(topic_k) + '_EPOCH' + str(EPOCH) + '_word'+ str(slim_words_amount) + now.strftime(\"_%y%m%d_%H%M\"), sparse_ptd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwt = scipy.sparse.load_npz('sparse_pwt_topic48_EPOCH30_word10020_201126_0310.npz').A\n",
    "ptd = scipy.sparse.load_npz('sparse_ptd_topic48_EPOCH30_word10020_201126_0310.npz').A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(ptd[:,i].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(topic_k):\n",
    "    print(pwt[:,i].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(topic_k):\n",
    "    topic_top10_words_index = np.flip(np.argsort(pwt[:,i]))[:10]\n",
    "    topic_top10_words = [slim_words[index] for index in topic_top10_words_index]\n",
    "    print('Topic ', i, ': ', ' '.join(topic_top10_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.4-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}