{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# load doc list\n",
    "with open('doc_list.txt') as f:\n",
    "    doc_list = f.read().splitlines()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 14955/14955 [01:00<00:00, 245.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# load doc from list\n",
    "if load_from_file != True:\n",
    "  docs_counter = []\n",
    "  words = set()\n",
    "  for doc in tqdm(doc_list):\n",
    "      with open('docs/' + doc + '.txt') as f:\n",
    "          doc_words = f.read().split()\n",
    "          docs_counter.append(Counter(doc_words))\n",
    "          words = words.union(set(doc_words))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load query list\n",
    "with open('query_list.txt') as f:\n",
    "    query_list = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 3173.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# load query from list\n",
    "queries = []\n",
    "queries_words = set()\n",
    "for query in tqdm(query_list):\n",
    "    with open('queries/' + query + '.txt') as f:\n",
    "        query_words = f.read().split()\n",
    "        queries.append(query_words)\n",
    "        if load_from_file == False:\n",
    "            words = words.union(set(query_words))\n",
    "            queries_words = queries_words.union(set(query_words))\n",
    "\n",
    "if load_from_file:\n",
    "    # load query words from file\n",
    "    with open('query_word_list.txt') as f:\n",
    "        queries_words = f.read().split()\n",
    "else:\n",
    "    # save query words\n",
    "    with open('query_word_list.txt', 'w') as f:\n",
    "        f.write(' '.join(queries_words))\n",
    "    \n",
    "    queries_words = list(queries_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_from_file:\n",
    "    # load words dict from file\n",
    "    with open('word_list.txt') as f:\n",
    "        words = f.read().split()\n",
    "else:\n",
    "    # save words\n",
    "    with open('word_list.txt', 'w') as f:\n",
    "        f.write(' '.join(words))\n",
    "\n",
    "    words = list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14955 111449 226\n"
     ]
    }
   ],
   "source": [
    "docs_amount = len(doc_list)\n",
    "words_amount = len(words)\n",
    "query_word_amount = len(queries_words)\n",
    "\n",
    "print(docs_amount, words_amount, query_word_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document length\n",
    "if load_from_file:\n",
    "    docs_len = np.load('docs_len.npy')\n",
    "else:\n",
    "    docs_len = []\n",
    "\n",
    "    for j in tqdm(range(docs_amount)):\n",
    "        docs_len.append(sum(docs_counter[j].values()))\n",
    "\n",
    "    docs_len = np.array(docs_len)\n",
    "    np.save('docs_len', docs_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all words count in documents and probability\n",
    "if load_from_file:\n",
    "    cwd = scipy.sparse.load_npz('cwd.npz')\n",
    "    pwd = scipy.sparse.load_npz('pwd.npz')\n",
    "else:\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    cwd_data = []\n",
    "    pwd_data = []\n",
    "\n",
    "    for j in tqdm(range(docs_amount)):\n",
    "        doc_len = docs_len[j]\n",
    "\n",
    "        for i in range(words_amount):\n",
    "            word_count = docs_counter[j][words[i]]\n",
    "            if word_count != 0:\n",
    "                indices.append(i)\n",
    "                cwd_data.append(word_count)\n",
    "                pwd_data.append(word_count / doc_len)\n",
    "        indptr.append(len(indices))\n",
    "\n",
    "    cwd = scipy.sparse.csr_matrix((cwd_data, indices, indptr), dtype=np.float32).transpose()\n",
    "    pwd = scipy.sparse.csr_matrix((pwd_data, indices, indptr), dtype=np.float32).transpose()\n",
    "\n",
    "    scipy.sparse.save_npz('cwd', cwd)\n",
    "    scipy.sparse.save_npz('pwd', pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "111449it [00:13, 8365.09it/s]10020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# process slim words\n",
    "if load_from_file:\n",
    "    with open('slim_word_list_10000.txt') as f:\n",
    "        slim_words = f.read().split()\n",
    "else:\n",
    "    words_count_list = []\n",
    "    for word_row in tqdm(cwd):\n",
    "        words_count_list.append(word_row.sum())\n",
    "        \n",
    "    most_word_index = np.flip(np.argsort(words_count_list))\n",
    "\n",
    "    slim_words_amount = 10000\n",
    "    slim_words = []\n",
    "    for i in range(slim_words_amount):\n",
    "        slim_words.append(words[most_word_index[i]])\n",
    "    \n",
    "    slim_words = slim_words + queries_words\n",
    "    slim_words = list(set(slim_words))\n",
    "\n",
    "    slim_words_amount = len(slim_words)\n",
    "\n",
    "    # save slim words\n",
    "    with open('slim_word_list_10000.txt', 'w') as f:\n",
    "        f.write(' '.join(slim_words))\n",
    "\n",
    "# update slim words amount\n",
    "slim_words_amount = len(slim_words)\n",
    "print(slim_words_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "type(slim_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 14955/14955 [01:44<00:00, 143.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# slim words count in documents and probability\n",
    "if load_from_file:\n",
    "    slim_cwd = scipy.sparse.load_npz('slim_cwd_10000.npz').A\n",
    "    slim_pwd = scipy.sparse.load_npz('slim_pwd_10000.npz').A\n",
    "else:\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    cwd_data = []\n",
    "    pwd_data = []\n",
    "\n",
    "    for j in tqdm(range(docs_amount)):\n",
    "        doc_len = docs_len[j]\n",
    "        for i in range(slim_words_amount):\n",
    "            word_count = docs_counter[j][slim_words[i]]\n",
    "            if word_count != 0:\n",
    "                indices.append(i)\n",
    "                cwd_data.append(word_count)\n",
    "                pwd_data.append(word_count / doc_len)\n",
    "        indptr.append(len(indices))\n",
    "\n",
    "    slim_cwd = scipy.sparse.csr_matrix((cwd_data, indices, indptr), dtype=np.float32).transpose()\n",
    "    slim_pwd = scipy.sparse.csr_matrix((pwd_data, indices, indptr), dtype=np.float32).transpose()\n",
    "\n",
    "    scipy.sparse.save_npz('slim_cwd_10000', slim_cwd)\n",
    "    scipy.sparse.save_npz('slim_pwd_10000', slim_pwd)\n",
    "\n",
    "    slim_cwd = slim_cwd.A\n",
    "    slim_pwd = slim_pwd.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10020/10020 [00:01<00:00, 5400.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# background language model\n",
    "bg = []\n",
    "bg_model_cd = docs_len.sum()\n",
    "\n",
    "for word_row in tqdm(slim_cwd):\n",
    "    bg.append(word_row.sum() / bg_model_cd)\n",
    "\n",
    "bg = np.array(bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def E_step():\n",
    "#     ptwd_CD = np.matmul(pwt, ptd) # Common Denominator\n",
    "#     for i in range(slim_words_amount):\n",
    "#         for j in range(docs_amount):\n",
    "#             if ptwd_CD[i][j] != 0:\n",
    "#                 for k in range(topic_k):\n",
    "#                     ptwd[k][i][j] = pwt[i][k] * ptd[k][j] / ptwd_CD[i][j]\n",
    "#             else:\n",
    "#                 ptwd[:,i,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def nb_E_step(pwt, ptd, cwd, topic_amount, word_amount, doc_amount):\n",
    "    # empty matrix\n",
    "    ptwd = np.empty((topic_amount, word_amount, doc_amount))\n",
    "\n",
    "    # Common Denominator\n",
    "    # ptwd_CD = np.dot(pwt, ptd) \n",
    "\n",
    "    for i in range(word_amount):\n",
    "        for j in range(doc_amount):\n",
    "            if cwd[i][j] != 0: \n",
    "                ptwd_CD = 0\n",
    "                for k in range(topic_amount):\n",
    "                    single_ptwd = pwt[i][k] * ptd[k][j]\n",
    "                    ptwd[k][i][j] = single_ptwd\n",
    "                    ptwd_CD += single_ptwd\n",
    "                if ptwd_CD != 0:\n",
    "                    for k in range(topic_amount):\n",
    "                        ptwd[k][i][j] /= ptwd_CD\n",
    "                else:\n",
    "                    ptwd[:,i,j] = 0\n",
    "            else:\n",
    "                ptwd[:,i,j] = 0\n",
    "    return ptwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @jit\n",
    "# def M_step():\n",
    "#     # p(w/t)\n",
    "#     for k in range(topic_k):\n",
    "#         single_wt = np.multiply(cwd, ptwd[k])\n",
    "#         single_wt_sum = single_wt.sum()\n",
    "#         if single_wt_sum != 0:\n",
    "#             for i in range(len(words)):\n",
    "#                 pwt[i][k] = single_wt[i].sum() / single_wt_sum\n",
    "#         else:\n",
    "#             for i in range(len(words)):\n",
    "#                 pwt[i][k] = 0\n",
    "    \n",
    "#     for i in range(len(pwt)):\n",
    "#         pwt[i] /= pwt[i].sum()\n",
    "\n",
    "#     # p(t/d)\n",
    "#     for k in range(topic_k):\n",
    "#         single_k_cwd_ptwd = np.multiply(cwd, ptwd[k])\n",
    "#         for j in range(len(docs)):\n",
    "#             if docs_len[j] != 0:\n",
    "#                 ptd[k][j] = single_k_cwd_ptwd[:,j].sum() / docs_len[j]\n",
    "#             else:\n",
    "#                 ptd[k][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit\n",
    "def M_step(times):\n",
    "    for k in range(topic_k):\n",
    "        single_topic_wd = np.multiply(cwd, ptwd[k])\n",
    "\n",
    "        # p(w/t)\n",
    "        single_wt_sum = single_topic_wd.sum()\n",
    "        if single_wt_sum != 0:\n",
    "            for i in range(slim_words_amount):\n",
    "                pwt[i][k] = single_topic_wd[i].sum() / single_wt_sum\n",
    "        else:\n",
    "            pwt[:,k] = 1 / slim_words_amount\n",
    "\n",
    "        # p(t/d)\n",
    "        for j in range(docs_amount):\n",
    "            ptd[k][j] = single_topic_wd[:,j].sum() / docs_len[j]\n",
    "    \n",
    "    # # norm to 1\n",
    "    # for k in range(topic_k):\n",
    "    #     if np.isnan(pwt[:,k].sum()):\n",
    "    #         print(times, \"norm \", k)\n",
    "    #     pwt[:,k] /= pwt[:,k].sum()\n",
    "    for j in range(docs_amount):\n",
    "        deno = ptd[:,j].sum()\n",
    "        if deno != 0:\n",
    "            ptd[:,j] /= deno\n",
    "        else:\n",
    "            ptd[:,j].fill(1 / topic_k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def nb_M_step(ptwd, cwd, docs_len, topic_amount, word_amount, doc_amount):\n",
    "    # empty matrix\n",
    "    pwt = np.empty((word_amount, topic_amount))\n",
    "    ptd = np.empty((topic_amount, doc_amount))\n",
    "\n",
    "    for k in range(topic_amount):\n",
    "        single_topic_wd = np.multiply(cwd, ptwd[k])\n",
    "\n",
    "        # p(w/t)\n",
    "        single_wt_sum = single_topic_wd.sum()\n",
    "        if single_wt_sum != 0:\n",
    "            for i in range(word_amount):\n",
    "                pwt[i][k] = single_topic_wd[i].sum() / single_wt_sum\n",
    "        else:\n",
    "            pwt[:,k] = 1 / slim_words_amount\n",
    "\n",
    "        # p(t/d)\n",
    "        for j in range(doc_amount):\n",
    "            ptd[k][j] = single_topic_wd[:,j].sum() / docs_len[j]\n",
    "            # ptd[k][j] = single_topic_wd[:,j].sum() / cwd[:,j].sum()\n",
    "    \n",
    "    # # norm to 1\n",
    "    # for k in range(topic_k):\n",
    "    #     if np.isnan(pwt[:,k].sum()):\n",
    "    #         print(times, \"norm \", k)\n",
    "    #     pwt[:,k] /= pwt[:,k].sum()\n",
    "    # for j in range(doc_amount):\n",
    "    #     deno = ptd[:,j].sum()\n",
    "    #     if deno != 0:\n",
    "    #         ptd[:,j] /= deno\n",
    "    #     else:\n",
    "    #         ptd[:,j].fill(1 / topic_amount) \n",
    "    \n",
    "    return pwt, ptd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(times):\n",
    "    loss = np.multiply(cwd, np.log(np.matmul(pwt, ptd))).sum()\n",
    "    print(\"\\nStep\", times, \"loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def nb_loss(times, cwd, pwt, ptd):\n",
    "    loss = np.multiply(cwd, np.log(np.dot(pwt, ptd))).sum()\n",
    "    print(\"\\nStep\", times, \"loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic\n",
    "\n",
    "topic_k = 48\n",
    "EPOCH = 30\n",
    "alpha = 0.7\n",
    "beta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM Step Initial (normal)\n",
    "pwt = np.random.random(size = (slim_words_amount, topic_k))\n",
    "\n",
    "for k in range(topic_k):\n",
    "    pwt[:,k] /= pwt[:,k].sum()\n",
    "\n",
    "ptd = np.full((topic_k, docs_amount), 1 / topic_k)\n",
    "\n",
    "ptwd = np.empty((topic_k, slim_words_amount, docs_amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 30/30 [1:31:07<00:00, 182.26s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(EPOCH)):\n",
    "    # nb_E_step(pwt, ptd, cwd, topic_amount, word_amount, doc_amount)\n",
    "    ptwd = nb_E_step(pwt, ptd, slim_cwd, topic_k, slim_words_amount, docs_amount)\n",
    "    # nb_M_step(ptwd, cwd, docs_len, topic_amount, word_amount, doc_amount)\n",
    "    pwt, ptd = nb_M_step(ptwd, slim_cwd, docs_len, topic_k, slim_words_amount, docs_amount)\n",
    "    # nb_loss(times, cwd, pwt, ptd)\n",
    "    # nb_loss(i + 1, slim_cwd, pwt, ptd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "plsa_EM_final = np.matmul(pwt, ptd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5042, 14955)"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "plsa_EM_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 100/100 [03:07<00:00,  1.87s/it]\n"
     ]
    }
   ],
   "source": [
    "queries_result = []\n",
    "\n",
    "for query in tqdm(queries):\n",
    "    query_result = []\n",
    "    for doc_index in range(docs_amount):\n",
    "        plsa_result = 1\n",
    "        for word in query:\n",
    "            word_index = slim_words.index(word)\n",
    "            unigram_pwd = slim_pwd[word_index][doc_index]\n",
    "            plsa_result = plsa_result * (alpha * unigram_pwd + beta * plsa_EM_final[word_index][doc_index] + (1 - alpha - beta) * bg[word_index])\n",
    "            \n",
    "            # plsa_result = plsa_result * (alpha * unigram_pwd + (1 - alpha - beta) * bg[word_index])\n",
    "        query_result.append(plsa_result)\n",
    "    queries_result.append(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort and export result\n",
    "sim_df = pd.DataFrame(queries_result)\n",
    "sim_df = sim_df.transpose()\n",
    "sim_df.index = doc_list\n",
    "sim_df.columns = query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                        301           302           303           304  \\\n",
       "FBIS3-1001     2.093690e-11  1.061319e-15  4.588218e-16  1.046446e-14   \n",
       "FBIS3-10014    2.738967e-11  9.201556e-16  4.931589e-16  1.051770e-14   \n",
       "FBIS3-10035    3.988780e-09  1.641297e-15  2.165017e-14  3.117691e-14   \n",
       "FBIS3-1007     2.804547e-11  6.129312e-16  4.047519e-15  1.013780e-14   \n",
       "FBIS3-10082    9.782491e-10  3.892662e-16  5.615827e-16  1.455064e-14   \n",
       "...                     ...           ...           ...           ...   \n",
       "LA123190-0062  1.365360e-11  9.120537e-16  4.394229e-16  1.188231e-14   \n",
       "LA123190-0065  1.800242e-10  6.566182e-15  4.932551e-16  8.809623e-14   \n",
       "LA123190-0069  4.836468e-12  2.658358e-16  4.584159e-16  9.968444e-15   \n",
       "LA123190-0089  5.962427e-12  2.836151e-16  1.592784e-15  7.253025e-14   \n",
       "LA123190-0117  4.950521e-12  3.018232e-16  4.031475e-16  1.008970e-14   \n",
       "\n",
       "                        305           306           307           308  \\\n",
       "FBIS3-1001     8.949977e-09  3.627352e-11  2.990166e-12  3.262359e-12   \n",
       "FBIS3-10014    8.672704e-09  8.421297e-13  2.796414e-12  3.416711e-12   \n",
       "FBIS3-10035    8.650170e-09  3.154298e-13  3.225514e-12  2.048372e-11   \n",
       "FBIS3-1007     7.371243e-09  1.705485e-11  3.654089e-12  1.076838e-11   \n",
       "FBIS3-10082    7.234331e-09  2.121981e-13  2.972856e-12  3.290567e-12   \n",
       "...                     ...           ...           ...           ...   \n",
       "LA123190-0062  1.456308e-08  3.300053e-08  2.611248e-12  4.311811e-12   \n",
       "LA123190-0065  1.380302e-08  5.055919e-12  2.249842e-11  6.810621e-12   \n",
       "LA123190-0069  6.329533e-09  1.357133e-13  3.009999e-12  8.135422e-12   \n",
       "LA123190-0089  1.531723e-08  4.314870e-13  2.951151e-11  4.995984e-12   \n",
       "LA123190-0117  2.656924e-08  2.129570e-11  2.795207e-12  3.822597e-12   \n",
       "\n",
       "                        309           310  ...           391       392  \\\n",
       "FBIS3-1001     1.845634e-10  1.332322e-18  ...  1.114330e-07  0.000026   \n",
       "FBIS3-10014    2.524660e-10  1.702369e-18  ...  1.677498e-07  0.000026   \n",
       "FBIS3-10035    3.064716e-10  1.759899e-17  ...  2.032955e-07  0.000026   \n",
       "FBIS3-1007     7.227223e-10  3.781390e-18  ...  2.800157e-07  0.000027   \n",
       "FBIS3-10082    8.018253e-09  1.153053e-18  ...  2.730137e-06  0.000027   \n",
       "...                     ...           ...  ...           ...       ...   \n",
       "LA123190-0062  1.814736e-10  2.339024e-18  ...  9.547320e-08  0.000026   \n",
       "LA123190-0065  1.015781e-08  2.282887e-18  ...  1.373443e-06  0.000027   \n",
       "LA123190-0069  2.619919e-10  2.499100e-18  ...  1.080084e-07  0.000027   \n",
       "LA123190-0089  2.086370e-10  8.075692e-19  ...  1.084485e-07  0.000029   \n",
       "LA123190-0117  1.451947e-10  1.085772e-18  ...  9.116881e-08  0.000027   \n",
       "\n",
       "                        393           394       395           396  \\\n",
       "FBIS3-1001     6.548698e-10  2.407025e-08  0.000077  1.499156e-14   \n",
       "FBIS3-10014    1.881855e-09  2.689350e-08  0.000060  2.100064e-14   \n",
       "FBIS3-10035    4.604987e-10  3.071536e-08  0.000093  1.560984e-13   \n",
       "FBIS3-1007     1.209230e-09  4.749952e-08  0.000067  3.204450e-14   \n",
       "FBIS3-10082    8.124797e-10  1.861541e-08  0.000052  2.053503e-14   \n",
       "...                     ...           ...       ...           ...   \n",
       "LA123190-0062  3.770814e-09  3.902565e-08  0.000071  2.306106e-14   \n",
       "LA123190-0065  5.865147e-08  6.052140e-08  0.000060  2.808765e-14   \n",
       "LA123190-0069  7.241724e-10  4.387245e-07  0.000089  4.988678e-12   \n",
       "LA123190-0089  2.834273e-09  5.983662e-08  0.000037  3.136418e-14   \n",
       "LA123190-0117  9.478028e-08  1.043562e-06  0.000040  3.191870e-14   \n",
       "\n",
       "                        397           398           399           400  \n",
       "FBIS3-1001     6.246422e-10  5.649983e-14  9.684796e-11  9.004595e-13  \n",
       "FBIS3-10014    6.314947e-10  4.756985e-14  9.564623e-11  1.213751e-14  \n",
       "FBIS3-10035    5.072881e-10  1.336222e-14  9.916510e-11  1.916171e-14  \n",
       "FBIS3-1007     9.382187e-10  1.846709e-14  9.475427e-11  7.704983e-14  \n",
       "FBIS3-10082    7.066957e-10  1.639840e-13  9.376218e-11  1.291680e-14  \n",
       "...                     ...           ...           ...           ...  \n",
       "LA123190-0062  1.527491e-09  7.430432e-15  2.272709e-10  1.181416e-13  \n",
       "LA123190-0065  1.965433e-09  1.480000e-14  9.823764e-11  1.781525e-14  \n",
       "LA123190-0069  8.394233e-10  8.338528e-15  9.277491e-11  1.335501e-14  \n",
       "LA123190-0089  2.553189e-09  7.724964e-15  1.055725e-10  2.630736e-14  \n",
       "LA123190-0117  4.779093e-09  7.842008e-15  1.006573e-10  9.200492e-14  \n",
       "\n",
       "[14955 rows x 100 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>301</th>\n      <th>302</th>\n      <th>303</th>\n      <th>304</th>\n      <th>305</th>\n      <th>306</th>\n      <th>307</th>\n      <th>308</th>\n      <th>309</th>\n      <th>310</th>\n      <th>...</th>\n      <th>391</th>\n      <th>392</th>\n      <th>393</th>\n      <th>394</th>\n      <th>395</th>\n      <th>396</th>\n      <th>397</th>\n      <th>398</th>\n      <th>399</th>\n      <th>400</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>FBIS3-1001</th>\n      <td>2.093690e-11</td>\n      <td>1.061319e-15</td>\n      <td>4.588218e-16</td>\n      <td>1.046446e-14</td>\n      <td>8.949977e-09</td>\n      <td>3.627352e-11</td>\n      <td>2.990166e-12</td>\n      <td>3.262359e-12</td>\n      <td>1.845634e-10</td>\n      <td>1.332322e-18</td>\n      <td>...</td>\n      <td>1.114330e-07</td>\n      <td>0.000026</td>\n      <td>6.548698e-10</td>\n      <td>2.407025e-08</td>\n      <td>0.000077</td>\n      <td>1.499156e-14</td>\n      <td>6.246422e-10</td>\n      <td>5.649983e-14</td>\n      <td>9.684796e-11</td>\n      <td>9.004595e-13</td>\n    </tr>\n    <tr>\n      <th>FBIS3-10014</th>\n      <td>2.738967e-11</td>\n      <td>9.201556e-16</td>\n      <td>4.931589e-16</td>\n      <td>1.051770e-14</td>\n      <td>8.672704e-09</td>\n      <td>8.421297e-13</td>\n      <td>2.796414e-12</td>\n      <td>3.416711e-12</td>\n      <td>2.524660e-10</td>\n      <td>1.702369e-18</td>\n      <td>...</td>\n      <td>1.677498e-07</td>\n      <td>0.000026</td>\n      <td>1.881855e-09</td>\n      <td>2.689350e-08</td>\n      <td>0.000060</td>\n      <td>2.100064e-14</td>\n      <td>6.314947e-10</td>\n      <td>4.756985e-14</td>\n      <td>9.564623e-11</td>\n      <td>1.213751e-14</td>\n    </tr>\n    <tr>\n      <th>FBIS3-10035</th>\n      <td>3.988780e-09</td>\n      <td>1.641297e-15</td>\n      <td>2.165017e-14</td>\n      <td>3.117691e-14</td>\n      <td>8.650170e-09</td>\n      <td>3.154298e-13</td>\n      <td>3.225514e-12</td>\n      <td>2.048372e-11</td>\n      <td>3.064716e-10</td>\n      <td>1.759899e-17</td>\n      <td>...</td>\n      <td>2.032955e-07</td>\n      <td>0.000026</td>\n      <td>4.604987e-10</td>\n      <td>3.071536e-08</td>\n      <td>0.000093</td>\n      <td>1.560984e-13</td>\n      <td>5.072881e-10</td>\n      <td>1.336222e-14</td>\n      <td>9.916510e-11</td>\n      <td>1.916171e-14</td>\n    </tr>\n    <tr>\n      <th>FBIS3-1007</th>\n      <td>2.804547e-11</td>\n      <td>6.129312e-16</td>\n      <td>4.047519e-15</td>\n      <td>1.013780e-14</td>\n      <td>7.371243e-09</td>\n      <td>1.705485e-11</td>\n      <td>3.654089e-12</td>\n      <td>1.076838e-11</td>\n      <td>7.227223e-10</td>\n      <td>3.781390e-18</td>\n      <td>...</td>\n      <td>2.800157e-07</td>\n      <td>0.000027</td>\n      <td>1.209230e-09</td>\n      <td>4.749952e-08</td>\n      <td>0.000067</td>\n      <td>3.204450e-14</td>\n      <td>9.382187e-10</td>\n      <td>1.846709e-14</td>\n      <td>9.475427e-11</td>\n      <td>7.704983e-14</td>\n    </tr>\n    <tr>\n      <th>FBIS3-10082</th>\n      <td>9.782491e-10</td>\n      <td>3.892662e-16</td>\n      <td>5.615827e-16</td>\n      <td>1.455064e-14</td>\n      <td>7.234331e-09</td>\n      <td>2.121981e-13</td>\n      <td>2.972856e-12</td>\n      <td>3.290567e-12</td>\n      <td>8.018253e-09</td>\n      <td>1.153053e-18</td>\n      <td>...</td>\n      <td>2.730137e-06</td>\n      <td>0.000027</td>\n      <td>8.124797e-10</td>\n      <td>1.861541e-08</td>\n      <td>0.000052</td>\n      <td>2.053503e-14</td>\n      <td>7.066957e-10</td>\n      <td>1.639840e-13</td>\n      <td>9.376218e-11</td>\n      <td>1.291680e-14</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>LA123190-0062</th>\n      <td>1.365360e-11</td>\n      <td>9.120537e-16</td>\n      <td>4.394229e-16</td>\n      <td>1.188231e-14</td>\n      <td>1.456308e-08</td>\n      <td>3.300053e-08</td>\n      <td>2.611248e-12</td>\n      <td>4.311811e-12</td>\n      <td>1.814736e-10</td>\n      <td>2.339024e-18</td>\n      <td>...</td>\n      <td>9.547320e-08</td>\n      <td>0.000026</td>\n      <td>3.770814e-09</td>\n      <td>3.902565e-08</td>\n      <td>0.000071</td>\n      <td>2.306106e-14</td>\n      <td>1.527491e-09</td>\n      <td>7.430432e-15</td>\n      <td>2.272709e-10</td>\n      <td>1.181416e-13</td>\n    </tr>\n    <tr>\n      <th>LA123190-0065</th>\n      <td>1.800242e-10</td>\n      <td>6.566182e-15</td>\n      <td>4.932551e-16</td>\n      <td>8.809623e-14</td>\n      <td>1.380302e-08</td>\n      <td>5.055919e-12</td>\n      <td>2.249842e-11</td>\n      <td>6.810621e-12</td>\n      <td>1.015781e-08</td>\n      <td>2.282887e-18</td>\n      <td>...</td>\n      <td>1.373443e-06</td>\n      <td>0.000027</td>\n      <td>5.865147e-08</td>\n      <td>6.052140e-08</td>\n      <td>0.000060</td>\n      <td>2.808765e-14</td>\n      <td>1.965433e-09</td>\n      <td>1.480000e-14</td>\n      <td>9.823764e-11</td>\n      <td>1.781525e-14</td>\n    </tr>\n    <tr>\n      <th>LA123190-0069</th>\n      <td>4.836468e-12</td>\n      <td>2.658358e-16</td>\n      <td>4.584159e-16</td>\n      <td>9.968444e-15</td>\n      <td>6.329533e-09</td>\n      <td>1.357133e-13</td>\n      <td>3.009999e-12</td>\n      <td>8.135422e-12</td>\n      <td>2.619919e-10</td>\n      <td>2.499100e-18</td>\n      <td>...</td>\n      <td>1.080084e-07</td>\n      <td>0.000027</td>\n      <td>7.241724e-10</td>\n      <td>4.387245e-07</td>\n      <td>0.000089</td>\n      <td>4.988678e-12</td>\n      <td>8.394233e-10</td>\n      <td>8.338528e-15</td>\n      <td>9.277491e-11</td>\n      <td>1.335501e-14</td>\n    </tr>\n    <tr>\n      <th>LA123190-0089</th>\n      <td>5.962427e-12</td>\n      <td>2.836151e-16</td>\n      <td>1.592784e-15</td>\n      <td>7.253025e-14</td>\n      <td>1.531723e-08</td>\n      <td>4.314870e-13</td>\n      <td>2.951151e-11</td>\n      <td>4.995984e-12</td>\n      <td>2.086370e-10</td>\n      <td>8.075692e-19</td>\n      <td>...</td>\n      <td>1.084485e-07</td>\n      <td>0.000029</td>\n      <td>2.834273e-09</td>\n      <td>5.983662e-08</td>\n      <td>0.000037</td>\n      <td>3.136418e-14</td>\n      <td>2.553189e-09</td>\n      <td>7.724964e-15</td>\n      <td>1.055725e-10</td>\n      <td>2.630736e-14</td>\n    </tr>\n    <tr>\n      <th>LA123190-0117</th>\n      <td>4.950521e-12</td>\n      <td>3.018232e-16</td>\n      <td>4.031475e-16</td>\n      <td>1.008970e-14</td>\n      <td>2.656924e-08</td>\n      <td>2.129570e-11</td>\n      <td>2.795207e-12</td>\n      <td>3.822597e-12</td>\n      <td>1.451947e-10</td>\n      <td>1.085772e-18</td>\n      <td>...</td>\n      <td>9.116881e-08</td>\n      <td>0.000027</td>\n      <td>9.478028e-08</td>\n      <td>1.043562e-06</td>\n      <td>0.000040</td>\n      <td>3.191870e-14</td>\n      <td>4.779093e-09</td>\n      <td>7.842008e-15</td>\n      <td>1.006573e-10</td>\n      <td>9.200492e-14</td>\n    </tr>\n  </tbody>\n</table>\n<p>14955 rows × 100 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "results/result_topic48_EPOCH30_a0.7_b0.1_word5042_201125_2113.txt\n"
     ]
    }
   ],
   "source": [
    "# save results\n",
    "now = datetime.datetime.now()\n",
    "save_filename = 'results/result' + '_' + 'topic' + str(topic_k) + '_EPOCH' + str(EPOCH) + '_a' + str(alpha) + '_b' + str(beta) + '_word'+ str(slim_words_amount) + now.strftime(\"_%y%m%d_%H%M\") + '.txt'\n",
    "print(save_filename)\n",
    "\n",
    "with open(save_filename, 'w') as f:\n",
    "    f.write('Query,RetrievedDocuments\\n')\n",
    "    for query in query_list:\n",
    "        f.write(query + \",\")\n",
    "        query_sim_df = sim_df[query].sort_values(ascending=False)\n",
    "        f.write(' '.join(query_sim_df[:1000].index.to_list()) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_pwt = scipy.sparse.csr_matrix(pwt)\n",
    "sparse_ptd = scipy.sparse.csr_matrix(ptd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('sparse_pwt' + '_' + 'topic' + str(topic_k) + '_EPOCH' + str(EPOCH) + '_word'+ str(slim_words_amount) + now.strftime(\"_%y%m%d_%H%M\"), sparse_pwt)\n",
    "scipy.sparse.save_npz('sparse_ptd' + '_' + 'topic' + str(topic_k) + '_EPOCH' + str(EPOCH) + '_word'+ str(slim_words_amount) + now.strftime(\"_%y%m%d_%H%M\"), sparse_ptd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwt = scipy.sparse.load_npz('sparse_pwt_topic48_EPOCH30_201125_0207.npz').A\n",
    "ptd = scipy.sparse.load_npz('sparse_ptd_topic48_EPOCH30_201125_0207.npz').A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.04477611940298507\n0.15999999999999998\n0.14022140221402213\n0.17361111111111108\n0.14285714285714285\n0.1698841698841699\n0.14054054054054055\n0.18957345971563985\n0.1330049261083744\n0.13876651982378851\n0.1235294117647059\n0.147887323943662\n0.14529914529914534\n0.12698412698412698\n0.12422360248447205\n0.14942528735632182\n0.15602836879432624\n0.15559772296015179\n0.15602836879432624\n0.1375968992248062\n0.13941018766756033\n0.2030075187969925\n0.159822633506844\n0.1307420494699647\n0.2985074626865672\n0.2030075187969925\n0.14625850340136057\n0.1473684210526316\n0.1310160427807487\n0.14625850340136057\n0.1473684210526316\n0.17647058823529416\n0.1488095238095238\n0.1746724890829694\n0.1746724890829694\n0.07499999999999998\n0.2113821138211382\n0.19827586206896552\n0.2\n0.0898876404494382\n0.14534883720930233\n0.1329113924050633\n0.15819209039548024\n0.1698924731182796\n0.1910828025477707\n0.14029850746268657\n0.18461538461538463\n0.17329545454545456\n0.22716049382716047\n0.16091954022988506\n0.22857142857142854\n0.17472118959107807\n0.21025641025641023\n0.19230769230769235\n0.19696969696969696\n0.17903930131004367\n0.1744186046511628\n0.1329113924050633\n0.17472118959107807\n0.1992337164750958\n0.2002176278563656\n0.25\n0.15633423180592992\n0.23048327137546465\n0.11971830985915494\n0.16607773851590105\n0.2063492063492064\n0.21578947368421053\n0.1859903381642512\n0.2007042253521127\n0.17647058823529416\n0.1171875\n0.1568627450980392\n0.16339869281045755\n0.1484375\n0.18103448275862072\n0.12917398945518455\n0.24884792626728108\n0.14549653579676675\n0.17931034482758623\n0.20454545454545453\n0.16462585034013605\n0.19249592169657426\n0.13571428571428573\n0.08571428571428572\n0.1564625850340136\n0.1564625850340136\n0.10628019323671498\n0.1301115241635688\n0.08974358974358974\n0.15776081424936386\n0.12365591397849462\n0.16434540389972144\n0.1068702290076336\n0.16666666666666666\n0.14589665653495443\n0.1380222104706504\n0.14814814814814817\n0.14598540145985403\n0.16884247171453437\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(ptd[:,i].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0000000000001894\n1.000000000000139\n1.0000000000002431\n1.0000000000002203\n1.0000000000002818\n1.0000000000001545\n1.0000000000001794\n1.0000000000000693\n1.0000000000001346\n1.000000000000212\n1.0000000000002442\n1.0000000000002427\n1.0000000000001878\n1.0000000000001168\n1.0000000000001161\n1.0000000000001636\n1.0000000000001248\n1.0000000000002793\n1.0000000000001825\n1.0000000000001847\n1.000000000000178\n1.0000000000002687\n1.0000000000001852\n1.0000000000001803\n1.0000000000002158\n1.0000000000001839\n1.0000000000002534\n1.0000000000001994\n1.0000000000001774\n1.0000000000001097\n1.0000000000001585\n1.0000000000001865\n1.0000000000002327\n1.0000000000000837\n1.0000000000001044\n1.000000000000179\n1.0000000000001266\n1.0000000000001728\n1.0000000000002345\n1.000000000000186\n1.0000000000001639\n1.0000000000001956\n1.0000000000001616\n1.0000000000001847\n1.0000000000002212\n1.0000000000001728\n1.0000000000002045\n1.0000000000001932\n"
     ]
    }
   ],
   "source": [
    "for i in range(topic_k):\n",
    "    print(pwt[:,i].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.4-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}